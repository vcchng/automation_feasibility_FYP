{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === Data Preprocessing Starts Here === #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa169a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG === #\n",
    "project = Path.cwd().parent\n",
    "target_folder = project / \"0 - data\"\n",
    "add_folder = project / \"4 - deployment\" \n",
    "file_path = target_folder / \"Task_Automation_Dataset.xlsx\"\n",
    "sheet = \"Task_Automation_Dataset\"\n",
    "\n",
    "df_raw = pd.read_excel(file_path, sheet_name=sheet)\n",
    "df_work = df_raw.copy(deep=True)\n",
    "\n",
    "target_col = \"Automation Suitable\"\n",
    "\n",
    "def print_table_nice(df, title):\n",
    "    print(f\"\\n===== {title} =====\")\n",
    "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False, floatfmt=\".2f\"))\n",
    "\n",
    "def get_class_variables_summary(df, target_col=None, unique_threshold=20, force_interval=None):\n",
    "    \"\"\"Return a SAS-like summary for class variables, excluding forced interval columns.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    force_interval = set(force_interval or [])\n",
    "    \n",
    "    class_vars = [\n",
    "        col for col in df.columns\n",
    "        if col not in force_interval and \n",
    "           (df[col].dtype == 'object' or df[col].nunique() <= unique_threshold)\n",
    "    ]\n",
    "    \n",
    "    summary = []\n",
    "    for col in class_vars:\n",
    "        role = \"TARGET\" if col == target_col else \"INPUT\"\n",
    "        levels = df[col].nunique(dropna=True)\n",
    "        missing = df[col].isna().sum()\n",
    "        \n",
    "        mode = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "        mode_pct = (df[col] == mode).mean() * 100 if mode is not None else None\n",
    "        \n",
    "        mode2 = None\n",
    "        mode2_pct = None\n",
    "        if levels > 1:\n",
    "            counts = df[col].value_counts(normalize=True) * 100\n",
    "            if len(counts) > 1:\n",
    "                mode2 = counts.index[1]\n",
    "                mode2_pct = counts.iloc[1]\n",
    "        \n",
    "        summary.append([role, col, levels, missing, mode, mode_pct, mode2, mode2_pct])\n",
    "    \n",
    "    return pd.DataFrame(summary, columns=[\n",
    "        \"Role\", \"Variable Name\", \"Levels\", \"Missing\", \n",
    "        \"Mode\", \"Mode Percentage\", \"Mode2\", \"Mode2 Percentage\"\n",
    "    ])\n",
    "\n",
    "def get_interval_variables_summary(df, target_col=None, unique_threshold=20, force_interval=None):\n",
    "    \"\"\"Return a SAS-like summary for interval variables with numeric stats.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    force_interval = set(force_interval or [])\n",
    "    \n",
    "    interval_vars = [\n",
    "        col for col in df.columns\n",
    "        if col in force_interval or (\n",
    "            pd.api.types.is_numeric_dtype(df[col]) and df[col].nunique() > unique_threshold\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    summary = []\n",
    "    for col in interval_vars:\n",
    "        series = df[col].dropna()\n",
    "        role = \"TARGET\" if col == target_col else \"INPUT\"\n",
    "        \n",
    "        mean_val = series.mean()\n",
    "        std_dev = series.std()\n",
    "        missing = df[col].isna().sum()\n",
    "        min_val = series.min()\n",
    "        median_val = series.median()\n",
    "        max_val = series.max()\n",
    "        skew_val = series.skew()\n",
    "        kurt_val = series.kurtosis()\n",
    "        \n",
    "        summary.append([\n",
    "            col, role, mean_val, std_dev, missing,\n",
    "            min_val, median_val, max_val, skew_val, kurt_val\n",
    "        ])\n",
    "    \n",
    "    return pd.DataFrame(summary, columns=[\n",
    "        \"Variable\", \"Role\", \"Mean\", \"Standard Deviation\",\n",
    "        \"Missing\", \"Minimum\", \"Median\", \"Maximum\",\n",
    "        \"Skewness\", \"Kurtosis\"\n",
    "    ])\n",
    "\n",
    "def summary_statistics(df, target_col=None):\n",
    "    force_interval = [\"Complexity (1-5)\"]\n",
    "\n",
    "    class_summary = get_class_variables_summary(\n",
    "        df,\n",
    "        target_col=target_col,\n",
    "        force_interval=force_interval\n",
    "    )\n",
    "    \n",
    "    interval_summary = get_interval_variables_summary(\n",
    "        df,\n",
    "        target_col=target_col,\n",
    "        force_interval=force_interval\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print_table_nice(class_summary, \"CLASS VARIABLES SUMMARY\")\n",
    "    print_table_nice(interval_summary, \"INTERVAL VARIABLES SUMMARY\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c627542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Outliers Functionality === #\n",
    "def detect_outliers_iqr(df, col):\n",
    "    series = pd.to_numeric(df[col], errors='coerce')\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    mask = (series < lower_bound) | (series > upper_bound)\n",
    "    print(f\"[{col}] Lower: {lower_bound:.2f}, Upper: {upper_bound:.2f}, Outliers: {mask.sum()}\")\n",
    "    return mask, lower_bound, upper_bound\n",
    "\n",
    "def fix_outliers_clip(df, col):\n",
    "    mask, lower, upper = detect_outliers_iqr(df, col)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce').clip(lower, upper)\n",
    "    print(f\"[{col}] Outliers clipped to range {lower:.2f}–{upper:.2f}\")\n",
    "    return df\n",
    "\n",
    "def run_outliers(df_work):\n",
    "    # Clip outliers for Time Taken (mins)\n",
    "    df_work = fix_outliers_clip(df_work, \"Time Taken (mins)\")\n",
    "\n",
    "    # Clip outliers for Error Rate (%) and also ensure 0–100%\n",
    "    df_work[\"Error Rate (%)\"] = df_work[\"Error Rate (%)\"].clip(0, 100)\n",
    "\n",
    "    return df_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Invalid Entry Handling Functionality === #\n",
    "NUM_WORDS = {\n",
    "    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
    "    \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10,\n",
    "    \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13, \"fourteen\": 14,\n",
    "    \"fifteen\": 15, \"sixteen\": 16, \"seventeen\": 17, \"eighteen\": 18,\n",
    "    \"nineteen\": 19, \"twenty\": 20, \"thirty\": 30, \"forty\": 40,\n",
    "    \"fifty\": 50, \"sixty\": 60, \"seventy\": 70, \"eighty\": 80, \"ninety\": 90,\n",
    "    \"hundred\": 100\n",
    "}\n",
    "\n",
    "def words_to_num(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    text = str(text).strip().lower()\n",
    "    if text.replace(\".\", \"\", 1).isdigit():\n",
    "        return float(text)\n",
    "    words = text.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in NUM_WORDS:\n",
    "            total += NUM_WORDS[w]\n",
    "    return total if total > 0 else None\n",
    "\n",
    "def fix_invalid_entries(df): # Fixed 2\n",
    "    # --- Time Taken (mins) ---\n",
    "    df[\"Time Taken (mins)\"] = df[\"Time Taken (mins)\"].apply(words_to_num)\n",
    "\n",
    "    # --- Complexity (1-5) ---\n",
    "    df[\"Complexity (1-5)\"] = df[\"Complexity (1-5)\"].apply(words_to_num)\n",
    "    df[\"Complexity (1-5)\"] = pd.to_numeric(df[\"Complexity (1-5)\"], errors='coerce')\n",
    "    df[\"Complexity (1-5)\"] = df[\"Complexity (1-5)\"].where(df[\"Complexity (1-5)\"].between(1, 5), pd.NA)\n",
    "\n",
    "    # Fill missing Complexity with mode\n",
    "    if df[\"Complexity (1-5)\"].isna().any():\n",
    "        mode_value = df[\"Complexity (1-5)\"].mode().iloc[0]\n",
    "        df[\"Complexity (1-5)\"].fillna(mode_value, inplace=True)\n",
    "    \n",
    "    # --- Error Rate (%) ---\n",
    "    df[\"Error Rate (%)\"] = df[\"Error Rate (%)\"].astype(str).str.strip().str.replace(\"%\", \"\", regex=False)\n",
    "    df[\"Error Rate (%)\"] = pd.to_numeric(df[\"Error Rate (%)\"], errors=\"coerce\")\n",
    "    df[\"Error Rate (%)\"] = df[\"Error Rate (%)\"].where(df[\"Error Rate (%)\"].between(0, 100), pd.NA)\n",
    "\n",
    "    print(\"Invalid entries fixed: numeric conversions, domain limits, and missing values handled.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ee35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Missing Values Functionality === #\n",
    "def handle_missing_values(df):\n",
    "    # Time Taken (mins)\n",
    "    time_median = df[\"Time Taken (mins)\"].median()\n",
    "    df[\"Time Taken (mins)\"].fillna(time_median, inplace=True)\n",
    "    print(f\"[Time Taken (mins)] Missing filled with median = {time_median}\")\n",
    "\n",
    "    # Frequency\n",
    "    freq_mode = df[\"Frequency\"].mode()[0]\n",
    "    df[\"Frequency\"].fillna(freq_mode, inplace=True)\n",
    "    print(f\"[Frequency] Missing filled with mode = {freq_mode}\")\n",
    "\n",
    "    # Department\n",
    "    dept_mode = df[\"Department\"].mode()[0]\n",
    "    df[\"Department\"].fillna(dept_mode, inplace=True)\n",
    "    print(f\"[Department] Missing filled with mode = {dept_mode}\")\n",
    "\n",
    "    # Error Rate (%)\n",
    "    err_mean = df[\"Error Rate (%)\"].mean()\n",
    "    df[\"Error Rate (%)\"].fillna(err_mean, inplace=True)\n",
    "    print(f\"[Error Rate (%)] Missing filled with mean = {err_mean}\")\n",
    "\n",
    "    # Rule-Based Indicator\n",
    "    RBI_mode = df[\"Rule-Based Indicator\"].mode()[0]\n",
    "    df[\"Rule-Based Indicator\"].fillna(RBI_mode, inplace=True)\n",
    "    print(f\"[Rule-Based Indicator] Missing filled with mode = {RBI_mode}\")\n",
    "\n",
    "    # Process Stability\n",
    "    stable_mode = df[\"Process Stability\"].mode()[0]\n",
    "    df[\"Process Stability\"].fillna(stable_mode, inplace=True)\n",
    "    print(f\"[Process Stability] Missing filled with mode = {stable_mode}\")\n",
    "\n",
    "    # Data Structure\n",
    "    DS_mode = df[\"Data Structure\"].mode()[0]\n",
    "    df[\"Data Structure\"].fillna(DS_mode, inplace=True)\n",
    "    print(f\"[Data Structure] Missing filled with mode = {DS_mode}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inconsistent Data Formatting Functionality === #\n",
    "def fix_typographical_errors(df):\n",
    "    tool_map = {\n",
    "        \"Excel\": \"Excel\", \"MS Excel\": \"Excel\", \"EXCEL\": \"Excel\",\n",
    "        \"Power BI\": \"Power BI\", \"Power Bi\": \"Power BI\",\n",
    "        \"SharePoint\": \"SharePoint\", \"SHAREPOINT\": \"SharePoint\",\n",
    "        \"Outlook\": \"Outlook\", \"MS Outlook\": \"Outlook\",\n",
    "        \"Microsoft Teams\": \"Microsoft Teams\", \"MS Teams\": \"Microsoft Teams\",\n",
    "        \"Google Sheets\": \"Google Sheets\", \"Google SHEETS\": \"Google Sheets\",\n",
    "        \"SAP\": \"SAP\", \"Zoom\": \"Zoom\", \"Jira\": \"Jira\", \"Slack\": \"Slack\",\n",
    "        \"Trello\": \"Trello\", \"Custom Portal\": \"Custom Portal\"\n",
    "    }\n",
    "    dept_map = {\n",
    "        \"IT\": \"IT\", \"hr\": \"Human Resource\", \"HR\": \"Human Resource\", \"human resource\": \"Human Resource\",\n",
    "        \"Sales\": \"Sales\", \"SALES\": \"Sales\", \"Marketing\": \"Marketing\", \"Compliance\": \"Compliance\",\n",
    "        \"Finance\": \"Finance\", \"Operations\": \"Operations\", \"Procurement\": \"Procurement\",\n",
    "        \"Admin\": \"Admin\", \"ADMIN\": \"Admin\", \"Customer Service\": \"Customer Service\"\n",
    "    }\n",
    "    freq_map = {\n",
    "        \"Daily\": \"Daily\", \"DAILY\": \"Daily\",\n",
    "        \"Weekly\": \"Weekly\",\n",
    "        \"Monthly\": \"Monthly\", \"MONTHLY\": \"Monthly\",\n",
    "        \"Quarterly\": \"Quarterly\",\n",
    "        \"Ad-Hoc\": \"Ad-Hoc\"\n",
    "    }\n",
    "    rbi_map = {\n",
    "        \"YES\": \"Yes\",\n",
    "        \"NO\": \"No\"\n",
    "    }\n",
    "    stable_map = {\n",
    "        \"HIGH\": \"High\",\n",
    "        \"LOW\": \"Low\"\n",
    "    }\n",
    "    ds_map = {\n",
    "        \"Not-structured\": \"Unstructured\",\n",
    "        \"SEMI-STRUCTURED\": \"Semi-Structured\",\n",
    "        \"CLEAN\": \"Structured\", \"structured\": \"Structured\"\n",
    "    }\n",
    "\n",
    "    # Safe mapping with NaN handling\n",
    "    for col, mapping in {\n",
    "        \"Tool Used\": tool_map,\n",
    "        \"Department\": dept_map,\n",
    "        \"Frequency\": freq_map,\n",
    "        \"Rule-Based Indicator\": rbi_map,\n",
    "        \"Process Stability\": stable_map,\n",
    "        \"Data Structure\": ds_map\n",
    "    }.items():\n",
    "        df[col] = df[col].apply(lambda x: mapping.get(str(x).strip(), x) if pd.notna(x) else x)\n",
    "\n",
    "    print(\"Typographical errors fixed: standardized Tool Used, Department, Frequency, Rule-Based Indicator, Process Stability, Data Structure\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Split and Standardization Functionality === #\n",
    "def split_and_standardize(df, target_col, test_size=0.2, random_state=42):\n",
    "    cols_to_standardize = [\"Time Taken (mins)\", \"Error Rate (%)\"]\n",
    "\n",
    "    # Split the dataset\n",
    "    train_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=df[target_col]  # keeps class balance similar\n",
    "    )\n",
    "\n",
    "    # Fit scaler on train_data only\n",
    "    scaler = StandardScaler()\n",
    "    train_df[cols_to_standardize] = scaler.fit_transform(train_df[cols_to_standardize])\n",
    "    test_df[cols_to_standardize] = scaler.transform(test_df[cols_to_standardize])\n",
    "\n",
    "    # Save scaler for deployment\n",
    "    joblib.dump(scaler, add_folder / \"scaler.pkl\")\n",
    "    print(\"Scaler saved to: 4 - deployment/scaler.pkl\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b628a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_42464\\1811030218.py:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Complexity (1-5)\"].fillna(mode_value, inplace=True)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_42464\\2533215133.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Time Taken (mins)\"].fillna(time_median, inplace=True)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_42464\\2533215133.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Frequency\"].fillna(freq_mode, inplace=True)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_42464\\2533215133.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Department\"].fillna(dept_mode, inplace=True)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_42464\\2533215133.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Error Rate (%)\"].fillna(err_mean, inplace=True)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_42464\\2533215133.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Rule-Based Indicator\"].fillna(RBI_mode, inplace=True)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_42464\\2533215133.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Process Stability\"].fillna(stable_mode, inplace=True)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_42464\\2533215133.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Data Structure\"].fillna(DS_mode, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid entries fixed: numeric conversions, domain limits, and missing values handled.\n",
      "[Time Taken (mins)] Missing filled with median = 91.0\n",
      "[Frequency] Missing filled with mode = Monthly\n",
      "[Department] Missing filled with mode = Compliance\n",
      "[Error Rate (%)] Missing filled with mean = 5.227835051546392\n",
      "[Rule-Based Indicator] Missing filled with mode = No\n",
      "[Process Stability] Missing filled with mode = Medium\n",
      "[Data Structure] Missing filled with mode = Structured\n",
      "Typographical errors fixed: standardized Tool Used, Department, Frequency, Rule-Based Indicator, Process Stability, Data Structure\n",
      "[Time Taken (mins)] Lower: -87.00, Upper: 275.00, Outliers: 7\n",
      "[Time Taken (mins)] Outliers clipped to range -87.00–275.00\n",
      "Scaler saved to: 4 - deployment/scaler.pkl\n",
      "\n",
      "\n",
      "===== CLASS VARIABLES SUMMARY =====\n",
      "+--------+----------------------+----------+-----------+---------------------------+-------------------+------------------+--------------------+\n",
      "| Role   | Variable Name        |   Levels |   Missing | Mode                      |   Mode Percentage | Mode2            |   Mode2 Percentage |\n",
      "+========+======================+==========+===========+===========================+===================+==================+====================+\n",
      "| INPUT  | Task Name            |       25 |         0 | Monthly Report Generation |              5.60 | Inventory Update |               5.40 |\n",
      "+--------+----------------------+----------+-----------+---------------------------+-------------------+------------------+--------------------+\n",
      "| INPUT  | Frequency            |        5 |         0 | Monthly                   |             24.00 | Quarterly        |              20.00 |\n",
      "+--------+----------------------+----------+-----------+---------------------------+-------------------+------------------+--------------------+\n",
      "| INPUT  | Tool Used            |       12 |         0 | Excel                     |             10.00 | Custom Portal    |               9.40 |\n",
      "+--------+----------------------+----------+-----------+---------------------------+-------------------+------------------+--------------------+\n",
      "| INPUT  | Department           |       10 |         0 | Compliance                |             14.80 | Sales            |              10.80 |\n",
      "+--------+----------------------+----------+-----------+---------------------------+-------------------+------------------+--------------------+\n",
      "| INPUT  | Rule-Based Indicator |        2 |         0 | No                        |             56.00 | Yes              |              44.00 |\n",
      "+--------+----------------------+----------+-----------+---------------------------+-------------------+------------------+--------------------+\n",
      "| INPUT  | Process Stability    |        3 |         0 | Medium                    |             38.40 | High             |              34.20 |\n",
      "+--------+----------------------+----------+-----------+---------------------------+-------------------+------------------+--------------------+\n",
      "| INPUT  | Data Structure       |        3 |         0 | Structured                |             47.60 | Semi-Structured  |              31.80 |\n",
      "+--------+----------------------+----------+-----------+---------------------------+-------------------+------------------+--------------------+\n",
      "| TARGET | Automation Suitable  |        2 |         0 | No                        |             54.20 | Yes              |              45.80 |\n",
      "+--------+----------------------+----------+-----------+---------------------------+-------------------+------------------+--------------------+\n",
      "\n",
      "===== INTERVAL VARIABLES SUMMARY =====\n",
      "+-------------------+--------+--------+----------------------+-----------+-----------+----------+-----------+------------+------------+\n",
      "| Variable          | Role   |   Mean |   Standard Deviation |   Missing |   Minimum |   Median |   Maximum |   Skewness |   Kurtosis |\n",
      "+===================+========+========+======================+===========+===========+==========+===========+============+============+\n",
      "| Task ID           | INPUT  | 250.50 |               144.48 |         0 |      1.00 |   250.50 |    500.00 |       0.00 |      -1.20 |\n",
      "+-------------------+--------+--------+----------------------+-----------+-----------+----------+-----------+------------+------------+\n",
      "| Time Taken (mins) | INPUT  |  94.56 |                54.79 |         0 |      6.00 |    91.00 |    275.00 |       0.40 |      -0.02 |\n",
      "+-------------------+--------+--------+----------------------+-----------+-----------+----------+-----------+------------+------------+\n",
      "| Complexity (1-5)  | INPUT  |   2.95 |                 1.38 |         0 |      1.00 |     3.00 |      5.00 |       0.09 |      -1.25 |\n",
      "+-------------------+--------+--------+----------------------+-----------+-----------+----------+-----------+------------+------------+\n",
      "| Error Rate (%)    | INPUT  |   5.23 |                 2.79 |         0 |      0.10 |     5.23 |     10.00 |      -0.05 |      -1.11 |\n",
      "+-------------------+--------+--------+----------------------+-----------+-----------+----------+-----------+------------+------------+\n",
      "\n",
      "Saved: 0 - data/train_clean.xlsx and 0 - data/test_clean.xlsx\n",
      "\n",
      "Cleaned dataset saved to: c:\\Users\\victo\\Desktop\\project\\0 - data\\Task_Automation_Dataset_Clean.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === Execute Data Preprocessing Pipeline === #\n",
    "def preprocess_pipeline(df, save_path= target_folder / \"Task_Automation_Dataset_Clean.xlsx\"):\n",
    "    # Step 1: Fix invalid entries\n",
    "    df = fix_invalid_entries(df)\n",
    "    \n",
    "    # Step 2: Handle missing values\n",
    "    df = handle_missing_values(df)\n",
    "    \n",
    "    # Step 3: Fix typographical errors\n",
    "    df = fix_typographical_errors(df)\n",
    "    \n",
    "    # Step 4: Handle outliers\n",
    "    df = run_outliers(df)\n",
    "    \n",
    "    # Step 5: Split dataset + standardize using train_data only\n",
    "    train_df, test_df = split_and_standardize(df, target_col)\n",
    "    \n",
    "    # Step 6: Summary statistics\n",
    "    summary_statistics(df, target_col)\n",
    "    \n",
    "    train_df.to_excel(target_folder / \"train_clean.xlsx\", index=False)\n",
    "    test_df.to_excel(target_folder / \"test_clean.xlsx\", index=False)\n",
    "    print(\"Saved: 0 - data/train_clean.xlsx and 0 - data/test_clean.xlsx\")\n",
    "\n",
    "    df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "    df.to_excel(save_path, index=False)\n",
    "    print(f\"\\nCleaned dataset saved to: {save_path}\")\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = preprocess_pipeline(df_raw)\n",
    "\n",
    "# === Data Preprocessing End Here === #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
